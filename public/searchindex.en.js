var relearn_searchindex = [
  {
    "breadcrumb": "",
    "content": "🚀 Containers Welcome to the Containers Knowledge Hub.\nThis section will guide you through the core concepts and practical usage of modern container technologies.\n📦 What You’ll Learn Docker → Building and running containerized applications Podman → Daemonless container management CRI-O \u0026 Low-Level Runtimes → Lightweight Kubernetes container runtime using OCI standards Kubernetes → Orchestrating containers at scale 📘 Why Containers? Containers provide:\nPortability across environments Lightweight and efficient deployments Faster development and scaling Integration with Kubernetes using runtimes like CRI-O and containerd",
    "description": "🚀 Containers Welcome to the Containers Knowledge Hub.\nThis section will guide you through the core concepts and practical usage of modern container technologies.\n📦 What You’ll Learn Docker → Building and running containerized applications Podman → Daemonless container management CRI-O \u0026 Low-Level Runtimes → Lightweight Kubernetes container runtime using OCI standards Kubernetes → Orchestrating containers at scale 📘 Why Containers? Containers provide:\nPortability across environments Lightweight and efficient deployments Faster development and scaling Integration with Kubernetes using runtimes like CRI-O and containerd",
    "tags": [],
    "title": "Containers",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Containers \u003e Docker",
    "content": "Topics 1. Linux Namespaces 2. Linux Cgroups 3. Unshare 4. chroot Introduction: What Are Linux Namespaces? By default, all processes on a Linux system share the same namespaces for things like process IDs, network, mounts, etc. This means:\nAll processes see the same list of processes. All processes share the same network interfaces and IP addresses. All processes see the same filesystem mount points. All processes share the same hostname. Why Do We Need Namespaces? Namespaces allow the Linux kernel to isolate and virtualize system resources so that a group of processes can have their own view of the system. This is key for:\nContainers (Docker, Kubernetes, OpenShift) Sandboxed environments Security isolation How to Create New Namespaces? Linux provides the unshare command and APIs to create new namespaces. When a process is created with new namespaces, it gets its own isolated view of resources.\nContainers like Docker also create new namespaces automatically when they launch.\nTypes of Linux Namespaces and What They Isolate Namespace Type What It Isolates Example of Isolation PID namespace Process IDs Separate process trees per container Mount namespace Filesystem mount points Containers have separate filesystem views Network namespace Network devices, IPs, ports Containers have their own network interfaces UTS namespace Hostname and domain name Containers have separate hostnames IPC namespace Interprocess communication Separate message queues and semaphores User namespace User and group IDs Containers can remap users for security Cgroup namespace View of cgroups Isolated view of resource groups Hands-On Lab 1. Verify All Processes Share the Same Namespaces by Default Run this on your current shell:\necho \"Current shell PID: $$\"\rls -l /proc/$$/ns/\rYou will see namespaces like pid, net, mnt, uts, etc. All processes started normally share these namespaces.\n2. Create a New Shell with New Namespaces Using unshare Run the following command to launch a new shell with new mount, UTS, IPC, network, and PID namespaces:\nsudo unshare --mount --uts --ipc --net --pid --fork /bin/bash\rThis starts a bash shell inside the new namespaces.\nInside this new shell:\nThe hostname is separate. Process IDs start fresh (your bash is PID 1). Mounted filesystems are isolated. Network interfaces are isolated. 3. Test Isolation Examples Change hostname:\nhostname my-container\recho \"New hostname: $(hostname)\"\rCheck process list (bash should be PID 1):\nps aux\rMount a new filesystem:\nmount -t tmpfs tmpfs /mnt\rmount | grep /mnt\rCheck network interfaces:\nip addr\rOpen another terminal and check the hostname, mounts, and processes there — they remain unaffected.\n4. Exit New Namespace Shell exit\rBonus: Explore Namespaces in Docker Containers Docker automatically creates new namespaces for each container.\nStart a container:\ndocker run -it --rm alpine sh\rInside container:\nhostname\rps aux\rls -l /proc/self/ns/\rOn host, find the container’s PID:\ndocker inspect --format '{{.State.Pid}}' \u003ccontainer_id_or_name\u003e\rls -l /proc/\u003ccontainer_pid\u003e/ns/\rSummary Without unshare or container runtimes, all processes share the same namespaces. Namespaces allow resource and process isolation needed by containers. unshare can be used to experiment with namespaces manually. Containers (Docker, Kubernetes) use namespaces to create isolated environments for workloads. Introduction to cgroups (Control Groups) While namespaces isolate what a process sees (e.g., process list, network), cgroups control how much resources a process or group of processes can use.\ncgroups allow you to limit, prioritize, and account for CPU, memory, disk I/O, and network bandwidth for processes.\nHow cgroups Work You create cgroups (control groups) and assign processes to them. You can configure limits, such as max CPU shares or memory limits, per cgroup. The Linux kernel enforces these limits, ensuring processes don’t exceed allocated resources. Common cgroups Controllers Controller Resource Controlled Description cpu CPU scheduling and usage Limits CPU time for processes memory Memory usage Limits RAM usage, triggers OOM blkio Block device I/O Controls disk read/write bandwidth net_cls Network traffic classification Controls and tags network traffic Hands-On Lab: Basic cgroups Usage 1. Check if cgroups are mounted mount | grep cgroup\rYou should see various cgroup controllers mounted, e.g., /sys/fs/cgroup/cpu, /sys/fs/cgroup/memory, etc.\n2. Create a cgroup and limit CPU sudo mkdir /sys/fs/cgroup/cpu/mygroup\recho 50000 | sudo tee /sys/fs/cgroup/cpu/mygroup/cpu.cfs_quota_us\recho 100000 | sudo tee /sys/fs/cgroup/cpu/mygroup/cpu.cfs_period_us\rThis limits the cgroup to 50% of a CPU core.\n3. Add a process to the cgroup Get the PID of a process (e.g., a bash shell):\npidof bash\rAdd the PID to the cgroup:\necho \u003cpid\u003e | sudo tee /sys/fs/cgroup/cpu/mygroup/cgroup.procs\r4. Verify the limits Check the cgroup usage and limits in /sys/fs/cgroup/cpu/mygroup.\nHow Namespaces and cgroups Work Together Namespaces isolate what processes see (virtualization of system resources). cgroups limit how much resources those processes can consume. Together, they form the backbone of container runtime isolation and resource management.\nCommon Docker Resource Control Flags Resource Docker Flag Description CPU --cpus, --cpu-shares, --cpu-quota, --cpu-period Limit CPU usage Memory --memory, --memory-swap Limit container memory usage Block I/O --device-read-bps, --device-write-bps Limit disk read/write bandwidth PIDs --pids-limit Limit number of processes in container CPU Limiting Examples Limit container to use 0.5 CPU cores docker run --rm -it --cpus=\"0.5\" ubuntu bash\rSet CPU shares (relative weight) docker run --rm -it --cpu-shares=512 ubuntu bash\rLimit CPU usage using quota and period Limit to 50% of one CPU: docker run --rm -it --cpu-quota=50000 --cpu-period=100000 ubuntu bash\rLimit Number of Processes (PIDs) docker run --rm -it --pids-limit=50 ubuntu bash\rSummary Namespaces provide resource isolation. cgroups provide resource control. Containers like Docker and Kubernetes use both heavily to provide secure, isolated, and resource-controlled environments. chroot vs unshare Aspect chroot unshare Purpose Change the root directory (/) for a process to isolate filesystem view. Create new namespaces to isolate various resources (PID, network, mount, etc.). Isolation type Filesystem isolation only. Can isolate multiple resources: PID, network, mount, UTS, IPC, user, etc. Security Limited; processes can escape chroot if privileged. Much stronger isolation; used for containers and sandboxing. Usage Run a command with a different root filesystem. Run a command with new kernel namespaces. Example chroot /new/root /bin/bash unshare --mount --pid --net /bin/bash Scope Filesystem namespace only. Multiple namespaces, depending on options. Container use Sometimes used historically for jail-like environments, but insufficient alone. Core kernel primitive for containers and sandboxing. Privilege Usually requires root. Usually requires root for some namespaces (e.g., network), but user namespaces allow unprivileged use. Summary chroot changes only the filesystem root for a process. unshare creates new namespaces, providing isolation for many system resources, not just filesystem. unshare is much more powerful and is the basis for modern container runtimes. chroot can be part of isolation but is often not secure enough alone. Docker Examples: Running Processes in Shared Namespaces 1. Share PID Namespace Between Containers Allows containers to see and interact with each other’s processes.\n# Start a container in detached mode docker run -dit --name container1 ubuntu sleep 1000 # Start another container sharing PID namespace with container1 docker run -dit --name container2 --pid=container:container1 ubuntu sleep 1000 # Exec into container2 and see processes (should see container1's processes too) docker exec -it container2 bash ps aux\r2. Share Network Namespace Between Containers Start container1 normally docker run -dit --name net1 ubuntu sleep 1000\rStart container2 sharing network namespace of net1 docker run -dit --name net2 --net=container:net1 ubuntu sleep 1000\rInside net2, check IP address (should be same as net1) docker exec -it net2 ip addr\r3. Share IPC Namespace Between Containers docker run -dit --name ipc1 ubuntu sleep 1000 docker run -dit --name ipc2 --ipc=container:ipc1 ubuntu sleep 1000\rCheck IPC namespace inode inside ipc2 docker exec ipc2 ls -l /proc/self/ns/ipc\r4. Share UTS Namespace Between Containers docker run -dit --name uts1 ubuntu sleep 1000 docker run -dit --name uts2 --uts=container:uts1 ubuntu sleep 1000\rChange hostname in uts1 container docker exec uts1 hostname shared-host\rCheck hostname in uts2 container (should be “shared-host”) docker exec uts2 hostname\r5. Share Mount Namespace Between Containers (Experimental) docker run -dit --name mount1 ubuntu sleep 1000 docker run -dit --name mount2 --mount=container:mount1 ubuntu sleep 1000\r6. Share Host PID Namespace docker run -it --pid=host ubuntu bash\rNow ps aux inside container shows all host processes ps aux\rTasks to do Install docker",
    "description": "Topics 1. Linux Namespaces 2. Linux Cgroups 3. Unshare 4. chroot Introduction: What Are Linux Namespaces? By default, all processes on a Linux system share the same namespaces for things like process IDs, network, mounts, etc. This means:\nAll processes see the same list of processes. All processes share the same network interfaces and IP addresses. All processes see the same filesystem mount points. All processes share the same hostname. Why Do We Need Namespaces? Namespaces allow the Linux kernel to isolate and virtualize system resources so that a group of processes can have their own view of the system. This is key for:",
    "tags": [],
    "title": "part 01",
    "uri": "/containers/container01/index.html"
  },
  {
    "breadcrumb": "Containers \u003e Docker",
    "content": "Open Container Initiative (OCI) and Container Standards What is OCI? The Open Container Initiative (OCI) is an open governance project under the Linux Foundation started in 2015. It creates open standards for container formats and runtimes to ensure interoperability and portability across container platforms.\nKey OCI Specifications OCI Image Format Specification\nDefines a standard image format including layers, manifests, and metadata so images are portable across tools.\nOCI Runtime Specification\nDefines how to run containers using namespaces, cgroups, hooks, and other OS features.\nOCI Distribution Specification\nDefines how images are pushed and pulled from registries.\nWhy OCI Matters Before OCI, different tools had incompatible container formats and runtimes leading to vendor lock-in and fragmentation. OCI solves this by standardizing image and runtime formats.\nRelated Concepts Docker Image: Initially a proprietary format; now OCI compatible. runc: Reference OCI runtime implementation. containerd: Daemon managing container lifecycle and image transfer, built on OCI specs. CRI-O: Kubernetes container runtime that uses OCI standards. Kubernetes: Container orchestration platform supporting OCI images and runtimes. Summary Table Term Description OCI Open standards for container images and runtimes. OCI Image Spec Defines container image format. OCI Runtime Spec Defines container runtime behavior. Docker Image Container image format compatible with OCI. runc Reference OCI runtime implementation. containerd Container runtime daemon implementing OCI specs. CRI-O Kubernetes runtime using OCI standards. Container Runtimes and Image Formats Explained Container Runtimes A container runtime is the software responsible for running containers according to the container image and runtime specifications. It handles creating the container, setting up namespaces, cgroups, and other OS-level features.\n1. runc What is it?\nrunc is the reference implementation of the OCI Runtime Specification. Role:\nIt is a lightweight CLI tool to spawn and run containers using Linux namespaces, cgroups, and other kernel features. Usage:\nrunc itself is mostly used as a low-level component by other container engines. Example:\nDocker and containerd use runc under the hood to create and run containers. 2. containerd What is it?\ncontainerd is a daemon that manages the entire container lifecycle: image transfer, container execution, storage, and supervision. Role:\nActs as an industry-standard container runtime daemon that uses runc for low-level container execution. Features: Pull and manage container images. Manage container snapshots and storage. Manage container execution and lifecycle. Usage:\nWidely used by Docker (as its runtime backend) and Kubernetes (via CRI plugins). 3. CRI-O What is it?\nCRI-O is an OCI-compliant container runtime specifically designed for Kubernetes. Role:\nProvides a lightweight runtime focused solely on Kubernetes’ Container Runtime Interface (CRI). Features: Supports OCI images and runtimes. Integrates tightly with Kubernetes. Avoids unnecessary components to reduce overhead. Usage:\nAn alternative to containerd for Kubernetes environments. Container Image Formats: OCI vs Docker Docker Image Format Originally developed by Docker Inc. Used proprietary format with JSON manifests and layers. Early versions had some incompatibilities with other runtimes. OCI Image Format Created by the Open Container Initiative (OCI) to standardize container images. Based on the Docker image format but with a well-defined spec for manifests, layers, and configuration. Ensures that images built by any OCI-compliant tool can run anywhere. Key Differences Feature Docker Image Format OCI Image Format Origin Docker Inc. Open Container Initiative (Linux Foundation) Specification Initially proprietary Open, community-driven specification Compatibility Supported by Docker and many runtimes Supported by Docker, containerd, Podman, CRI-O Focus Docker ecosystem Interoperability and standardization Why It Matters The OCI image format promotes portability and interoperability across container tools and platforms. Most modern container tools now support OCI images as a standard. Summary Table Term Description runc Reference OCI runtime; low-level container executor using Linux kernel features. containerd Container runtime daemon managing lifecycle, image transfer, storage; uses runc internally. CRI-O Lightweight Kubernetes-focused OCI runtime implementing CRI interface. Docker Image Original Docker container image format, now largely compatible with OCI images. OCI Image Open standard container image format promoting portability and interoperability. Container Runtimes: High-Level vs Low-Level Explained What is a Container Runtime? A container runtime is software that runs containers by managing container lifecycle, image handling, and execution.\nHigh-Level vs Low-Level Container Runtimes Aspect High-Level Runtime Low-Level Runtime (OCI Runtime) Role Manages container lifecycle, image pull, storage, networking, etc. Creates and runs containers using kernel features like namespaces and cgroups Examples containerd, CRI-O, Docker Engine runc, crun, kata-runtime Responsibilities - Pull and manage images- Manage container lifecycle (start, stop)- Handle storage and snapshots- Provide API for orchestration tools - Spawn container processes- Set up namespaces, cgroups, capabilities- Execute container according to OCI runtime spec Interaction Calls low-level runtime to actually start containers Runs as subprocess invoked by high-level runtime Usage in Kubernetes containerd and CRI-O implement Kubernetes CRI interface Usually runc (or alternative OCI runtimes) used by containerd or CRI-O to execute containers Installation Runs as daemon/service CLI tools or plugins Complexity More complex, handles many aspects of container lifecycle Lightweight, focused on execution only How They Work Together The high-level runtime manages everything except the actual container start. When a container needs to start, the high-level runtime calls the low-level runtime. The low-level runtime (like runc) creates the container process using Linux kernel primitives (namespaces, cgroups). This separation enables modularity, flexibility, and easier maintenance. Examples High-Level Runtime Low-Level Runtime (OCI Runtime) Notes Docker Engine runc Docker uses containerd and runc containerd runc or crun Kubernetes often uses this combo CRI-O runc or crun Kubernetes-focused lightweight runtime Why This Design? Modularity: Easier to swap out low-level runtimes without changing lifecycle management. Standardization: runc follows OCI spec as a reference runtime. Performance \u0026 Security: Alternative runtimes like crun, kata-runtime, or gvisor can be plugged in for specialized needs. Summary Table Component Description High-Level Runtime Container lifecycle, image management, API Low-Level Runtime Runs containers using kernel features runc Reference low-level OCI runtime containerd Popular high-level runtime daemon CRI-O Kubernetes-specific high-level runtime crun Lightweight alternative low-level runtime kata-runtime VM-based low-level runtime for enhanced security Using Multiple Low-Level Container Runtimes Can We Have Multiple Low-Level Runtimes? Yes, it is possible and common to have multiple low-level runtimes installed and configured on the same system.\nWhy Use Multiple Low-Level Runtimes? Different workloads may require different runtimes for: Performance (e.g., crun is faster and lighter than runc) Security (e.g., kata-runtime provides VM-level isolation) Specialized environments or sandboxing How to Configure Multiple Runtimes in containerd In the containerd configuration file (/etc/containerd/config.toml), you can define multiple runtimes:\n[plugins.cri.containerd.runtimes.runc] runtime_type = \"io.containerd.runc.v2\" runtime_engine = \"/usr/bin/runc\" [plugins.cri.containerd.runtimes.crun] runtime_type = \"io.containerd.runc.v2\" runtime_engine = \"/usr/bin/crun\" [plugins.cri.containerd.runtimes.kata] runtime_type = \"io.containerd.kata.v2\" runtime_engine = \"/usr/bin/kata-runtime\"\rSelecting Runtime Per Container or Pod When creating containers or pods, specify which runtime to use. In Kubernetes, this is done via RuntimeClass. apiVersion: node.k8s.io/v1 kind: RuntimeClass metadata: name: kata handler: kata\rCommands to Verify Installed Container Runtimes Check installed low-level runtimes (common locations) which runc which crun which kata-runtime\rCheck running containerd service systemctl status containerd ps aux | grep containerd\rVerify configured runtimes in containerd config cat /etc/containerd/config.toml | grep -A 5 runtimes\rCheck default runtime Docker uses docker info | grep -i runtime\rkubectl get runtimeclass kubectl get runtimeclass\rInspect container runtime for a specific pod kubectl get pod \u003cpod-name\u003e -o jsonpath='{.spec.runtimeClassName}'\rContainers Under the Hood: The Core Concepts 1. Linux Namespaces – Isolation Namespaces provide process isolation by creating separate instances of global system resources for each container. This ensures that containers have their own view of:\nPID namespace: Separate process IDs inside each container (process 1 inside container ≠ host process 1) Mount namespace: Isolated filesystem mount points Network namespace: Independent network interfaces, IP addresses, and ports UTS namespace: Separate hostname and domain name IPC namespace: Isolated inter-process communication mechanisms (message queues, semaphores) User namespace: Maps user and group IDs, allowing privilege isolation Cgroup namespace: Isolated view of control groups (resource groups) 2. Control Groups (cgroups) – Resource Management Cgroups limit and prioritize the resources (CPU, memory, disk I/O, network) that containers can use. This prevents one container from hogging all system resources and impacting others.\nYou can set limits like max CPU shares or memory usage The kernel enforces these limits at runtime Important for multi-tenant environments and resource fairness 3. Union File Systems (OverlayFS, AUFS, etc.) – Efficient Storage Containers use union filesystems to build layered images efficiently:\nBase OS layers + application layers stacked without duplication Containers share read-only layers; only container-specific changes are stored separately (copy-on-write) This reduces storage space and speeds up container start times 4. Container Runtime The container runtime (e.g., containerd, CRI-O, or Docker Engine) is responsible for:\nCreating namespaces and cgroups Setting up network interfaces (via CNI plugins) Mounting the filesystem layers Starting and managing container processes 5. Networking Containers get isolated network stacks:\nVirtual Ethernet interfaces (veth pairs) connect container namespace to host network bridge IP addresses assigned per container Network policies enforce traffic rules (firewalls, routing) 6. Process Lifecycle Containers run as regular processes on the host but inside isolated namespaces and resource limits:\nPID 1 inside the container is typically the main application process Signals and process control are managed inside namespaces, isolated from the host Summary Containers combine Linux kernel features (namespaces + cgroups + union filesystems) with runtime tooling to create lightweight, isolated, and resource-controlled environments for running applications consistently anywhere.\nOCI Compliance and Practical Container Checks This guide helps verify if container images and containers follow OCI standards, with hands-on commands.\n1. Checking OCI Compliance for Images Using skopeo: skopeo inspect docker://docker.io/library/alpine:latest\nLook for \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\" → confirms OCI Image Spec.\nUsing umoci: umoci unpack --image docker.io/library/alpine:latest oci-layout-dir\nCheck cat oci-layout-dir/oci-layout → presence of oci-layout, index.json, blobs/ confirms OCI layout.\nDocker / Podman CLI:\ndocker image inspect alpine --format '{{.Os}} {{.Architecture}}'\ndocker image inspect alpine | jq '.[0].MediaType' → application/vnd.oci.image.manifest.v1+json → OCI-compliant.\n2. Checking OCI Compliance for Containers / Runtimes Using runc: runc spec --rootless then runc run mycontainer → container follows OCI Runtime Spec.\nInspect Running Container:\ndocker inspect -f '{{.State.Pid}}' \u003ccontainer_name\u003e → get PID\nlsns -p \u003cpid\u003e → check namespaces\ncat /proc/\u003cpid\u003e/cgroup → check resource limits / cgroups\nUsing containerd: sudo ctr containers list and sudo ctr containers info \u003ccontainer-id\u003e → OCI-compliant.\nKubernetes RuntimeClass:\nkubectl get pod \u003cpod-name\u003e -o jsonpath='{.spec.runtimeClassName}'\nkubectl get pod \u003cpod-name\u003e -o jsonpath='{.status.containerStatuses[0].containerID}' → runc, crun, kata-runtime → OCI runtime compliance.\n3. Hands-On Linux Kernel Isolation Namespaces with unshare:\nsudo unshare --fork --pid --mount-proc bash → ps aux shows PID namespace isolated\nsudo unshare --mount bash → mount | grep proc shows mount namespace isolated\nFilesystem Isolation with chroot:\nmkdir -p /tmp/myroot/{bin,lib,lib64}\ncp /bin/bash /tmp/myroot/bin/\ncopy required libraries from ldd /bin/bash sudo chroot /tmp/myroot /bin/bash → ls / shows isolated filesystem\nInspect cgroups inside container:\ncat /sys/fs/cgroup/memory/memory.limit_in_bytes\ncat /sys/fs/cgroup/cpu/cpu.shares\ncat /proc/\u003cpid\u003e/cgroup\n4. Summary Table Target Tool / Command How to Check Image skopeo / umoci MediaType=application/vnd.oci.image.manifest.v1+json Image Docker / Podman docker image inspect Container runc / crun runc starts container via config.json Running container lsns / /proc//cgroup Namespaces \u0026 cgroups indicate OCI runtime Kubernetes pod kubectl + RuntimeClass RuntimeClass references OCI-compliant runtime ✅ These commands let you verify OCI compliance for images, containers, and runtimes, and practice Linux kernel isolation features used in containers.",
    "description": "Open Container Initiative (OCI) and Container Standards What is OCI? The Open Container Initiative (OCI) is an open governance project under the Linux Foundation started in 2015. It creates open standards for container formats and runtimes to ensure interoperability and portability across container platforms.\nKey OCI Specifications OCI Image Format Specification\nDefines a standard image format including layers, manifests, and metadata so images are portable across tools.\nOCI Runtime Specification\nDefines how to run containers using namespaces, cgroups, hooks, and other OS features.",
    "tags": [],
    "title": "Part 02",
    "uri": "/containers/conatiner02/index.html"
  },
  {
    "breadcrumb": "Containers \u003e Docker",
    "content": "Docker Architecture – Workflow +----------------+ Unix Socket / REST API +----------------+ | Docker Client | ----------------------------\u003e | Docker Daemon | | (CLI / GUI) | | (dockerd) | +----------------+ +----------------+ | | Uses container runtime (containerd) v +-----------------------+ | Container Runtime | | (containerd / CRI-O) | +-----------------------+ | | Creates \u0026 manages v +-----------------------+ | Containers | | (Running Apps) | +-----------------------+ ^ | Images | +-----------------------+ | Docker Registry | | (Docker Hub / Private)| +-----------------------+\rDocker CLI | v Docker Daemon (dockerd) | v containerd (high-level runtime) | v runc (low-level runtime, OCI-compliant) | v Linux Kernel (namespaces, cgroups, filesystem)\rComponents Explained 1. Docker Client (CLI) Interface to interact with Docker (docker run, docker build). Sends commands to Docker daemon via: Unix socket (/var/run/docker.sock) TCP/REST API (optional, for remote management) 2. Docker Daemon (dockerd) Background service managing containers, images, networks, and volumes. Receives commands from CLI. Delegates container creation to the container runtime. 3. Container Runtime Low-level manager responsible for running containers. Docker uses containerd by default. Handles: Image unpacking Container lifecycle Storage and logging Communicates with daemon via API or socket. 4 Low-level Runtime The low-level runtime is responsible for actually creating and running containers on the host OS. runc is the default low-level runtime used by Docker, containerd, and CRI-O. Responsibilities of the low-level runtime: Creating namespaces for process isolation Managing cgroups for resource limits Mounting filesystems for container file isolation Directly interacting with the Linux kernel to run containers It implements the OCI runtime specification, ensuring containers are standardized and portable. High-level runtimes like containerd or CRI-O call the low-level runtime to execute containers. 5. Docker Registries Store and distribute container images. Docker Hub: Public registry. Private registries: Internal organization use. Commands: docker pull, docker push. 6. Communication Flow User runs docker run nginx. CLI sends request via Unix socket to daemon. Daemon pulls image from registry (if needed). Daemon delegates container creation to containerd. containerd interacts with OS kernel to start the container. Daemon sends container info back to CLI. 7. Socket Exposure Unix socket: /var/run/docker.sock (local access, secure) TCP socket: tcp://0.0.0.0:2375 (remote access, insecure) Kubernetes CRI sockets: CRI-O: /var/run/crio/crio.sock containerd: /run/containerd/containerd.sock",
    "description": "Docker Architecture – Workflow +----------------+ Unix Socket / REST API +----------------+ | Docker Client | ----------------------------\u003e | Docker Daemon | | (CLI / GUI) | | (dockerd) | +----------------+ +----------------+ | | Uses container runtime (containerd) v +-----------------------+ | Container Runtime | | (containerd / CRI-O) | +-----------------------+ | | Creates \u0026 manages v +-----------------------+ | Containers | | (Running Apps) | +-----------------------+ ^ | Images | +-----------------------+ | Docker Registry | | (Docker Hub / Private)| +-----------------------+\rDocker CLI | v Docker Daemon (dockerd) | v containerd (high-level runtime) | v runc (low-level runtime, OCI-compliant) | v Linux Kernel (namespaces, cgroups, filesystem)\rComponents Explained 1. Docker Client (CLI) Interface to interact with Docker (docker run, docker build). Sends commands to Docker daemon via: Unix socket (/var/run/docker.sock) TCP/REST API (optional, for remote management) 2. Docker Daemon (dockerd) Background service managing containers, images, networks, and volumes. Receives commands from CLI. Delegates container creation to the container runtime. 3. Container Runtime Low-level manager responsible for running containers. Docker uses containerd by default. Handles: Image unpacking Container lifecycle Storage and logging Communicates with daemon via API or socket. 4 Low-level Runtime The low-level runtime is responsible for actually creating and running containers on the host OS. runc is the default low-level runtime used by Docker, containerd, and CRI-O. Responsibilities of the low-level runtime: Creating namespaces for process isolation Managing cgroups for resource limits Mounting filesystems for container file isolation Directly interacting with the Linux kernel to run containers It implements the OCI runtime specification, ensuring containers are standardized and portable. High-level runtimes like containerd or CRI-O call the low-level runtime to execute containers. 5. Docker Registries Store and distribute container images. Docker Hub: Public registry. Private registries: Internal organization use. Commands: docker pull, docker push. 6. Communication Flow User runs docker run nginx. CLI sends request via Unix socket to daemon. Daemon pulls image from registry (if needed). Daemon delegates container creation to containerd. containerd interacts with OS kernel to start the container. Daemon sends container info back to CLI. 7. Socket Exposure Unix socket: /var/run/docker.sock (local access, secure) TCP socket: tcp://0.0.0.0:2375 (remote access, insecure) Kubernetes CRI sockets: CRI-O: /var/run/crio/crio.sock containerd: /run/containerd/containerd.sock",
    "tags": [],
    "title": "Part 03",
    "uri": "/containers/container03/index.html"
  },
  {
    "breadcrumb": "Containers \u003e Docker",
    "content": "1. Container Related commands Commands Lifecycle docker run [OPTIONS] IMAGE [COMMAND] [ARG...] docker run -d -p 8080:80 --name webserver nginx docker start CONTAINER_NAME_OR_ID docker stop CONTAINER_NAME_OR_ID docker restart CONTAINER_NAME_OR_ID docker rm CONTAINER_NAME_OR_ID docker rm -f CONTAINER_NAME_OR_ID docker pause CONTAINER_NAME docker unpause CONTAINER_NAME docker kill CONTAINER_NAME\rMonitoring \u0026 Inspection docker ps docker ps -a docker inspect CONTAINER_NAME docker logs CONTAINER_NAME docker logs -f CONTAINER_NAME docker stats docker stats CONTAINER_NAME docker exec -it CONTAINER_NAME bash docker attach CONTAINER_NAME docker top CONTAINER_NAME docker exec CONTAINER_NAME env docker inspect -f '{{ .Mounts }}' CONTAINER_NAME\rManagement docker network inspect NETWORK_NAME docker network connect NETWORK_NAME CONTAINER_NAME docker network disconnect NETWORK_NAME CONTAINER_NAME docker run --network NETWORK_NAME IMAGE_NAME\rmisc docker run -it IMAGE_NAME bash docker run -d IMAGE_NAME docker run -d --name limited_container --memory=\"512m\" --cpus=\"1.0\" IMAGE_NAME docker ps -q -f ancestor=IMAGE_NAME\rInstall mysql in docker\nInstall nginx\nTry to connect from nginx container\nProejct Link\ncreate build ( war file) set up mysql Set up Tomcat Test if the app is able to persist the data Docker Images 1. What is a Docker Image A Docker image is a read-only template used to create containers. It contains: Application code Runtime Libraries Dependencies OS-level configurations (optional) Images are composed of layers, each layer representing a filesystem change. 2. Image Layers and Filesystem Docker images are built using layers, each layer represents a filesystem change. Layers are stacked on top of each other using union filesystems. Union filesystem allows layers to be combined into a single view for the container. Examples of union filesystems Docker uses: overlay2 (default on modern Linux) aufs (older systems) btrfs zfs How Layers Work Base layer: Usually an OS image like ubuntu or alpine. Intermediate layers: Created by RUN, COPY, ADD commands in Dockerfile. Top layer (container layer): Writable layer when container is running. Caching: Docker reuses unchanged layers to speed up builds. 3. Image Storage Purpose Images are stored locally to allow fast container creation without downloading every time. Stored in Docker’s storage driver directory (depends on filesystem/driver): Default for Linux with overlay2: /var/lib/docker/overlay2/ Each image layer is read-only; containers get a writable top layer on top. Comparison of Docker Storage Drivers / Filesystems Storage Driver Default on Features Pros Cons Use Case overlay2 Modern Linux OverlayFS-based union filesystem Fast, efficient, low storage overhead, supports copy-on-write Requires modern kernel (≥ 4.0) Recommended for most modern Docker setups aufs Older Linux Multi-layered union filesystem Supports multiple read-only layers, writable top layer Deprecated, slower than overlay2, not in mainline kernel Legacy systems btrfs Select distributions Copy-on-write filesystem, snapshots Snapshots \u0026 rollbacks, efficient storage, checksums for data integrity Requires btrfs kernel support, more complex Advanced use, snapshot \u0026 rollback capability zfs Select distributions Advanced filesystem with COW Snapshots, clones, compression, large-scale datasets Requires ZFS kernel support, more complex setup Enterprise setups, large storage, snapshot \u0026 clone heavy workflows vfs All Linux (fallback) Simple, non-union filesystem Simple, no kernel requirements, works everywhere No copy-on-write, poor performance, large storage use Testing, environments where no unionfs is available Key Points overlay2 is the modern default and most recommended for production Docker use. aufs is legacy and mostly phased out. btrfs and zfs provide advanced features like snapshots and cloning but require extra setup and kernel support. vfs is a simple fallback driver that doesn’t use copy-on-write; very slow and storage-heavy. All drivers (except vfs) support union filesystem concepts, enabling Docker image layers (read-only) and container writable layers efficiently. Copy-on-Write (CoW) in Docker What is Copy-on-Write? Copy-on-Write (CoW) is a storage optimization technique used by Docker’s storage drivers (overlay2, aufs, btrfs, zfs).\nInstead of duplicating files/layers, Docker uses references to existing read-only image layers.\nA copy is only made when a modification occurs.\nHow It Works Image Layers:\nDocker images are built in layers (e.g., base OS, updates, application code). These layers are read-only. Container Creation:\nWhen a container starts, Docker adds a thin writable layer (container layer) on top of the image layers. The container can write new files or modify existing ones in this writable layer. File Modification (Copy-on-Write Trigger):\nIf a process modifies a file from a lower (read-only) image layer: The file is first copied into the container’s writable layer. The process then modifies this copied version. The original file in the image layer remains unchanged. Example Image has /etc/config.txt in a read-only layer. Container modifies /etc/config.txt. Docker: Copies /etc/config.txt → to the writable container layer. Applies the change only in the container. Result: Image layer is unchanged, container sees the modified file. Benefits of Copy-on-Write Storage efficiency: No duplication unless needed. Fast container startup: Containers share image layers. Immutability: Image layers are never altered after creation. Isolation: Each container has its own writable layer. Drawbacks of Copy-on-Write Performance overhead: First write involves copying. Complexity: File operations are slightly slower than native filesystem writes. Driver dependency: Behavior differs across overlay2, aufs, btrfs, zfs. Storage Drivers Using CoW overlay2: Most common; efficient CoW at the file level. aufs: Older; also uses CoW at the file level. btrfs: Native CoW filesystem with advanced features (snapshots, rollback). zfs: Native CoW filesystem with snapshots and clones. vfs: Does not support CoW (copies files fully → slow and storage-heavy). 4. Image Lifecycle Commands Pulling and Searching # Pull an image from Docker Hub or private registry docker pull IMAGE_NAME[:TAG] # Search images on Docker Hub docker search IMAGE_NAME\rListing and Inspecting Images # List all local images docker images # Inspect image details (layers, environment variables, entrypoint) docker inspect IMAGE_NAME[:TAG] # View history of image layers docker history IMAGE_NAME[:TAG] # Inspect image metadata docker inspect IMAGE_NAME[:TAG] # Inspect layers of an image docker history IMAGE_NAME[:TAG] # Check environment variables and entrypoint docker inspect -f '{{ .Config.Env }}' IMAGE_NAME[:TAG] docker inspect -f '{{ .Config.Entrypoint }}' IMAGE_NAME[:TAG]\rTagging and Pushing images # Tag a local image for a registry docker tag IMAGE_NAME[:TAG] REGISTRY_URL/IMAGE_NAME[:TAG] # Push image to registry docker push REGISTRY_URL/IMAGE_NAME[:TAG]\rRemoving Images # Remove an image locally docker rmi IMAGE_NAME[:TAG] # Force remove image if in use docker rmi -f IMAGE_NAME[:TAG]\rConverting Container Changes to Images # Commit changes from a running container to a new image docker commit CONTAINER_NAME NEW_IMAGE_NAME[:TAG]\rWorking with Private Registries # Tag an image for private registry docker tag my-app:v1 myregistry.local:5000/my-app:v1 # Push image to private registry docker push myregistry.local:5000/my-app:v1 # Pull image from private registry docker pull myregistry.local:5000/my-app:v1\rSave and Load Docker Images as TAR Files # Save nginx latest image docker save -o nginx_latest.tar nginx:latest # Save Ubuntu 22.04 image docker save -o ubuntu_22.04.tar ubuntu:22.04\rLoad Docker Image from a TAR File # Load nginx image from tar docker load -i nginx_latest.tar # Load Ubuntu image from tar docker load -i ubuntu_22.04.tar\rTask Install nexus as a docker image Login to nexus Push your images to nexus Solution 🐳 Task: Install Nexus as a Docker Image and Push Custom Images 1️⃣ Run Nexus in Docker # Create a persistent volume for Nexus data mkdir -p ~/nexus-data \u0026\u0026 chmod 777 ~/nexus-data # Run Sonatype Nexus 3 as a container docker run -d --name nexus \\ -p 8081:8081 -p 5000:5000 \\ -v ~/nexus-data:/nexus-data \\ sonatype/nexus3\rNexus UI → http://localhost:8081 Default admin credentials → admin / password from ~/nexus-data/admin.password Create a Private Docker Registry in Nexus Login to Nexus UI (http://localhost:8081) Go to Administration → Repositories → Create repository Select docker (hosted) Name it (e.g., docker-hosted) HTTP Port → 5000 Save\rNow Nexus is acting as a Docker registry at http://localhost:5000. Configure Docker to Trust Nexus Registry Edit /etc/docker/daemon.json: { \"insecure-registries\": [\"localhost:5000\"] }\rRestart Docker: sudo systemctl restart docker\rLogin to Nexus Docker Registry docker login localhost:5000 # Enter Nexus admin credentials (or user credentials you created)\rTag and Push an Image to Nexus # Pull a base image docker pull alpine:latest # Tag the image for Nexus docker tag alpine:latest localhost:5000/my-alpine:1.0 # Push to Nexus registry docker push localhost:5000/my-alpine:1.0\rVerify Image in Nexus Go to Nexus UI → Browse → Repositories → docker-hosted\rYou should see my-alpine:1.0 pushed successfully. Docker Registries Docker Hub (default public registry) Harbor (open-source private registry) JFrog Artifactory (enterprise registry) GitLab Container Registry (integrated with GitLab) AWS Elastic Container Registry (ECR) Azure Container Registry (ACR) Google Artifact Registry / Container Registry (GCR)",
    "description": "1. Container Related commands Commands Lifecycle docker run [OPTIONS] IMAGE [COMMAND] [ARG...] docker run -d -p 8080:80 --name webserver nginx docker start CONTAINER_NAME_OR_ID docker stop CONTAINER_NAME_OR_ID docker restart CONTAINER_NAME_OR_ID docker rm CONTAINER_NAME_OR_ID docker rm -f CONTAINER_NAME_OR_ID docker pause CONTAINER_NAME docker unpause CONTAINER_NAME docker kill CONTAINER_NAME\rMonitoring \u0026 Inspection docker ps docker ps -a docker inspect CONTAINER_NAME docker logs CONTAINER_NAME docker logs -f CONTAINER_NAME docker stats docker stats CONTAINER_NAME docker exec -it CONTAINER_NAME bash docker attach CONTAINER_NAME docker top CONTAINER_NAME docker exec CONTAINER_NAME env docker inspect -f '{{ .Mounts }}' CONTAINER_NAME\rManagement docker network inspect NETWORK_NAME docker network connect NETWORK_NAME CONTAINER_NAME docker network disconnect NETWORK_NAME CONTAINER_NAME docker run --network NETWORK_NAME IMAGE_NAME\rmisc docker run -it IMAGE_NAME bash docker run -d IMAGE_NAME docker run -d --name limited_container --memory=\"512m\" --cpus=\"1.0\" IMAGE_NAME docker ps -q -f ancestor=IMAGE_NAME\rInstall mysql in docker",
    "tags": [],
    "title": "Part 04",
    "uri": "/containers/container04/index.html"
  },
  {
    "breadcrumb": "Containers \u003e Docker",
    "content": "Docker Restart Policies \u0026 Types of Containers 1. Docker Restart Policies Restart policies define how Docker should handle container restarts when they exit or the Docker daemon restarts.\nTypes of Restart Policies no (default)\nContainer does not restart automatically. Example: docker run --restart=no nginx\ron-failure\nRestarts container only if it exits with a non-zero exit code (error). Optionally, you can limit retries (e.g., on-failure:5). Example: docker run --restart=on-failure:3 myapp\ralways\nAlways restarts the container, regardless of exit status. If stopped manually, it will restart after Docker daemon restarts. Example: docker run --restart=always redis\runless-stopped\nSimilar to always, but if you manually stop the container, it will not restart after daemon reboot. Example: docker run --restart=unless-stopped postgres\r2. Types of Containers Docker containers can be categorized based on their purpose and usage:\n2.1 System / Service Containers Long-running services (like databases, web servers, monitoring tools). Typically use restart policies (always, unless-stopped). Examples: mysql, nginx, prometheus. 2.2 Application Containers Encapsulate application code and dependencies. Often run a single app or microservice. Examples: mycompany/api:latest, node:20-app. 2.3 Ephemeral / Debug Containers Short-lived containers for testing, debugging, or one-off tasks. Do not usually need restart policies. Examples: docker run --rm -it ubuntu bash\r# Docker Storage\rDocker storage allows containers to store and share data beyond their ephemeral writable layer. By default, container data is lost when the container is removed. To persist or share data, Docker provides different storage mechanisms.\r---\r## 1. Types of Docker Storage\r### 1.1 Container Writable Layer\r- Every container gets a **thin writable layer** on top of the image layers.\r- Temporary: deleted when the container is removed.\r- Good for short-lived, non-persistent data.\r- **Limitation**: tied to container lifecycle.\r### 1.2 Volumes (Recommended)\r- Managed by Docker (`/var/lib/docker/volumes/` on host).\r- Independent of container lifecycle.\r- Can be shared between multiple containers.\r- Can be backed by plugins (e.g., NFS, cloud storage).\r- Example:\r```bash\rdocker volume create mydata\rdocker run -d -v mydata:/app/data nginx\r1.3 Bind Mounts Maps a directory or file from the host filesystem into the container. Flexible: can use any path on host. Good for local development and sharing configs/logs. Example: docker run -d -v /host/path:/container/path nginx\rDocker Volumes Docker volumes provide a way to persist data outside of containers’ writable layers.\nThey are managed by Docker and are the preferred mechanism for data persistence.\n1. Why Use Volumes? Data persistence beyond container lifecycle Sharing data between multiple containers Easier backup and restore Better performance compared to bind mounts Managed by Docker (stored in /var/lib/docker/volumes/) 2. Types of Docker Storage Volumes Managed by Docker Stored in /var/lib/docker/volumes/ Best for persistence Bind Mounts Maps host path → container path Stored anywhere on host filesystem Good for local development tmpfs Mounts Stored only in memory Non-persistent Good for sensitive or temporary data 3. Common Docker Volume Commands Create a Volume\ndocker volume create myvolume\nList Volumes\ndocker volume ls\nInspect a Volume\ndocker volume inspect myvolume\nRemove a Volume\ndocker volume rm myvolume\nRemove All Unused Volumes\ndocker volume prune\n4. Using Volumes with Containers Mount a Volume\ndocker run -d --name mycontainer -v myvolume:/data busybox\nBind Mount Example\ndocker run -d --name mybind -v /host/data:/container/data busybox\ntmpfs Mount Example\ndocker run -d --name mytmp --mount type=tmpfs,destination=/app/tmp busybox\n5. Backup \u0026 Restore Volumes Backup a Volume\ndocker run --rm -v myvolume:/data -v $(pwd):/backup busybox tar cvf /backup/backup.tar /data\nRestore a Volume\ndocker run --rm -v myvolume:/data -v $(pwd):/backup busybox tar xvf /backup/backup.tar -C /\n6. Volume Drivers Docker supports different volume drivers, e.g.:\nlocal (default, stores data on local filesystem) nfs (store data on remote NFS server) azurefile, aws ebs, gcp-pd (cloud storage integrations) Example:\ndocker volume create --driver local mylocalvol\ndocker volume create --driver vieux/sshfs -o sshcmd=user@host:/path -o password=pass mysshvolume\n7. Best Practices Use named volumes instead of anonymous volumes Prefer volumes over bind mounts for portability Use tmpfs for sensitive, ephemeral data Regularly prune unused volumes For production, consider driver-based storage (NFS, cloud) Docker Networking Docker networking allows containers to communicate with each other, the host system, and external networks.\nIt is essential for microservices and multi-container applications.\n1. Default Networks When Docker is installed, it creates these default networks:\nbridge (default for standalone containers) host (container shares host network stack) none (isolated, no networking) Check networks: docker network ls\nInspect a network: docker network inspect bridge\n2. Types of Docker Networks Bridge Network (default)\nContainers get their own IP NAT used for external communication Best for single-host setups Example:\ndocker run -d --name web --network bridge nginx Host Network\nContainer shares host’s network namespace No port mapping needed Higher performance, but less isolation Example:\ndocker run -d --network host nginx None Network\nNo external connectivity Only loopback interface available Useful for security and testing Example:\ndocker run -d --network none nginx User-defined Bridge Network\nAllows custom isolated networks Containers can talk by name Example:\ndocker network create mynetwork\ndocker run -d --name db --network mynetwork mysql\ndocker run -d --name app --network mynetwork nginx Overlay Network (Swarm/Kubernetes)\nMulti-host networking Uses VXLAN tunneling Suitable for distributed apps Example:\ndocker network create -d overlay myoverlay Macvlan Network\nAssigns MAC address from host’s network Container appears as a physical device Used for legacy applications Example:\ndocker network create -d macvlan --subnet=192.168.1.0/24 --gateway=192.168.1.1 -o parent=eth0 mymacvlan 3. Common Networking Commands List networks\ndocker network ls\nInspect a network\ndocker network inspect \u003cnetwork_name\u003e\nCreate a network\ndocker network create mynet\nRemove a network\ndocker network rm mynet\nConnect a container to a network\ndocker network connect mynet mycontainer\nDisconnect a container from a network\ndocker network disconnect mynet mycontainer\n4. Port Mapping By default, containers in bridge networks are isolated from the host.\nWe use -p or --publish to map host ports.\nExample:\ndocker run -d -p 8080:80 nginx\n(Maps host port 8080 → container port 80) 5. DNS \u0026 Service Discovery Docker provides built-in DNS for container name resolution. Containers in the same user-defined network can resolve each other by name. Example:\nping db from app container in the same network. 6. Best Practices Use user-defined bridge networks for multi-container apps. For multi-host deployments, use overlay networks. Use macvlan when containers need to appear as separate physical devices. Avoid exposing unnecessary ports to the host. Use docker-compose or Kubernetes for complex networking.",
    "description": "Docker Restart Policies \u0026 Types of Containers 1. Docker Restart Policies Restart policies define how Docker should handle container restarts when they exit or the Docker daemon restarts.\nTypes of Restart Policies no (default)\nContainer does not restart automatically. Example: docker run --restart=no nginx\ron-failure\nRestarts container only if it exits with a non-zero exit code (error). Optionally, you can limit retries (e.g., on-failure:5). Example: docker run --restart=on-failure:3 myapp\ralways",
    "tags": [],
    "title": "Part 05",
    "uri": "/containers/container05/index.html"
  },
  {
    "breadcrumb": "Containers \u003e Docker",
    "content": "Docker Security Docker security is about protecting containers, images, the Docker daemon, host system, and networks from vulnerabilities or misuse. It requires best practices at all layers.\n1. Image Security Use official/trusted images only. Keep images small \u0026 minimal (Alpine, distroless). Scan images for vulnerabilities: docker scan myimage:latest Enable Docker Content Trust (DCT) to sign \u0026 verify images: export DOCKER_CONTENT_TRUST=1 2. Container Security Run as non-root user: USER 1001 (in Dockerfile) Avoid –privileged containers (full host access). Use read-only filesystem: docker run –read-only nginx Limit resources (cgroups): docker run -m 512m –cpus=“1.0” nginx Drop capabilities: docker run –cap-drop=ALL –cap-add=NET_BIND_SERVICE nginx 3. Docker Daemon Security Prefer rootless mode for Docker. Protect /var/run/docker.sock (limit access). Use TLS for Docker API: dockerd –tlsverify –tlscacert=ca.pem –tlscert=server-cert.pem –tlskey=server-key.pem -H=0.0.0.0:2376 4. Network Security Use user-defined networks for isolation. Encrypt overlay networks for multi-host. Restrict exposed ports (-p flag). Apply firewall rules (iptables, ufw). 5. Host Security Keep OS \u0026 Docker updated. Enable AppArmor or SELinux. Restrict root access on host. For strong isolation → run containers inside VMs. 6. Security Tools Docker Bench for Security – Audits configs: docker run -it –net host –pid host –cap-add audit_control -v /var/lib:/var/lib -v /var/run/docker.sock:/var/run/docker.sock -v /usr/lib/systemd:/usr/lib/systemd -v /etc:/etc docker/docker-bench-security Trivy / Clair / Anchore – Image scanning. Falco – Runtime monitoring. 7. Seccomp Profiles Restrict Linux syscalls with seccomp. Example: docker run –security-opt seccomp=/path/to/profile.json nginx 8. Rootless Docker Run daemon \u0026 containers without root → reduces attack surface. Install: curl -fsSL https://get.docker.com/rootless | sh 9. Best Practices (Quick Checklist ✅) Use minimal, signed images Run containers as non-root Drop privileges \u0026 capabilities Use TLS for API Patch host \u0026 Docker regularly Scan images before use Monitor containers at runtime Creating a Container with Full Host Access ⚠️ Warning: This setup gives the container root-level access to the host. Use only on safe environments or lab machines. Misuse can compromise the host system.\n1. Using Docker with --privileged This gives the container almost full control over the host.\ndocker run -it --rm --privileged \\ -v /:/host \\ --network host \\ ubuntu:22.04 /bin/bash\rInside the container: ls /host # Browse all host files chroot /host # Switch to host filesystem (optional)\r# Creating a Container with Full Host Access and Network Access ⚠️ Warning: This setup gives the container root-level access to the host, including full network visibility. Use only on safe environments or lab machines. Misuse can compromise the host system.\n1. Full Host Access with Docker --privileged docker run -it --rm --privileged \\ -v /:/host \\ --network host \\ ubuntu:22.04 /bin/bash\rDocker bench\nhttps://github.com/docker/docker-bench-security Aqua Security\nhttps://github.com/aquasecurity/",
    "description": "Docker Security Docker security is about protecting containers, images, the Docker daemon, host system, and networks from vulnerabilities or misuse. It requires best practices at all layers.\n1. Image Security Use official/trusted images only. Keep images small \u0026 minimal (Alpine, distroless). Scan images for vulnerabilities: docker scan myimage:latest Enable Docker Content Trust (DCT) to sign \u0026 verify images: export DOCKER_CONTENT_TRUST=1 2. Container Security Run as non-root user: USER 1001 (in Dockerfile) Avoid –privileged containers (full host access). Use read-only filesystem: docker run –read-only nginx Limit resources (cgroups): docker run -m 512m –cpus=“1.0” nginx Drop capabilities: docker run –cap-drop=ALL –cap-add=NET_BIND_SERVICE nginx 3. Docker Daemon Security Prefer rootless mode for Docker. Protect /var/run/docker.sock (limit access). Use TLS for Docker API: dockerd –tlsverify –tlscacert=ca.pem –tlscert=server-cert.pem –tlskey=server-key.pem -H=0.0.0.0:2376 4. Network Security Use user-defined networks for isolation. Encrypt overlay networks for multi-host. Restrict exposed ports (-p flag). Apply firewall rules (iptables, ufw). 5. Host Security Keep OS \u0026 Docker updated. Enable AppArmor or SELinux. Restrict root access on host. For strong isolation → run containers inside VMs. 6. Security Tools Docker Bench for Security – Audits configs: docker run -it –net host –pid host –cap-add audit_control -v /var/lib:/var/lib -v /var/run/docker.sock:/var/run/docker.sock -v /usr/lib/systemd:/usr/lib/systemd -v /etc:/etc docker/docker-bench-security Trivy / Clair / Anchore – Image scanning. Falco – Runtime monitoring. 7. Seccomp Profiles Restrict Linux syscalls with seccomp. Example: docker run –security-opt seccomp=/path/to/profile.json nginx 8. Rootless Docker Run daemon \u0026 containers without root → reduces attack surface. Install: curl -fsSL https://get.docker.com/rootless | sh 9. Best Practices (Quick Checklist ✅) Use minimal, signed images Run containers as non-root Drop privileges \u0026 capabilities Use TLS for API Patch host \u0026 Docker regularly Scan images before use Monitor containers at runtime Creating a Container with Full Host Access ⚠️ Warning: This setup gives the container root-level access to the host. Use only on safe environments or lab machines. Misuse can compromise the host system.",
    "tags": [],
    "title": "Part 06",
    "uri": "/containers/container06-/index.html"
  },
  {
    "breadcrumb": "Containers \u003e Docker",
    "content": "——————————- 1. DB Server Configuration ——————————- yum install mariadb-server -y systemctl enable mariadb systemctl start mariadb\rConfigure Database mysql -e \" CREATE DATABASE studentapp; USE studentapp; CREATE TABLE Students ( student_id INT NOT NULL AUTO_INCREMENT, student_name VARCHAR(100) NOT NULL, student_addr VARCHAR(100) NOT NULL, student_age VARCHAR(3) NOT NULL, student_qual VARCHAR(20) NOT NULL, student_percent VARCHAR(10) NOT NULL, student_year_passed VARCHAR(10) NOT NULL, PRIMARY KEY (student_id) ); GRANT ALL PRIVILEGES ON studentapp.* TO 'student'@'%' IDENTIFIED BY 'student@1'; FLUSH PRIVILEGES; \"\r——————————- 2. Tomcat (Application Server) ——————————- yum install java -y cd /root wget -qO- http://www-us.apache.org/dist/tomcat/tomcat-8/v8.5.27/bin/apache-tomcat-8.5.27.tar.gz | tar -xz cd apache-tomcat-8.5.27 rm -rf webapps/* wget https://github.com/cit-latex/stack/raw/master/mysql-connector-java-5.1.40.jar -O lib/mysql-connector-java-5.1.40.jar wget https://github.com/cit-latex/stack/raw/master/student.war -O webapps/student.war\rEdit context.xml (replace accordingly) sed -i '/\u003c\\/Context\u003e/i \\\u003cResource name=\"jdbc/TestDB\" auth=\"Container\" type=\"javax.sql.DataSource\" maxTotal=\"100\" maxIdle=\"30\" maxWaitMillis=\"10000\" username=\"student\" password=\"student@1\" driverClassName=\"com.mysql.jdbc.Driver\" url=\"jdbc:mysql://\u003cIP-ADDRESS-OF-DB-SERVER\u003e:3306/studentapp\"/\u003e' conf/context.xml ### Start Tomcat sh bin/startup.sh\r——————————- 3. Web Server (Apache HTTPD) ——————————- yum install httpd httpd-devel gcc -y cd /root wget -qO- http://www-eu.apache.org/dist/tomcat/tomcat-connectors/jk/tomcat-connectors-1.2.42-src.tar.gz | tar -xz cd tomcat-connectors-1.2.42-src/native ./configure --with-apxs=/usr/bin/apxs make make install ## Configure worker.properties (replace \u003cIP-ADDRESS-OF-TOMCAT-SERVER\u003e accordingly) cat \u003c\u003c EOF \u003e /etc/httpd/conf.d/worker.properties worker.list=tomcatA worker.tomcatA.type=ajp13 worker.tomcatA.host=\u003cIP-ADDRESS-OF-TOMCAT-SERVER\u003e worker.tomcatA.port=8009 EOF ### Configure mod_jk.conf cat \u003c\u003c EOF \u003e /etc/httpd/conf.d/mod_jk.conf LoadModule jk_module modules/mod_jk.so JkWorkersFile conf.d/worker.properties JkMount /student tomcatA JkMount /student/* tomcatA EOF ### Enable and start HTTPD systemctl enable httpd systemctl start httpd\r3 tier App in conatiners using docker compose Create necessary directory mkdir -p docker-app \u0026\u0026 cd docker-app\rCreate context.xml cat \u003c\u003c EOF \u003e context.xml \u003cContext\u003e \u003cResource name=\"jdbc/TestDB\" auth=\"Container\" type=\"javax.sql.DataSource\" maxTotal=\"100\" maxIdle=\"30\" maxWaitMillis=\"10000\" username=\"student\" password=\"student@1\" driverClassName=\"com.mysql.jdbc.Driver\" url=\"jdbc:mysql://mariadb:3306/studentapp\"/\u003e \u003c/Context\u003e EOF\rDownload mysql connector wget https://github.com/cit-latex/stack/raw/master/mysql-connector-java-5.1.40.jar -O mysql-connector-java-5.1.40.jar\rDownload student.war wget https://github.com/cit-latex/stack/raw/master/student.war -O student.war\rCreate worker.properties cat \u003c\u003c EOF \u003e worker.properties worker.list=tomcatA worker.tomcatA.type=ajp13 worker.tomcatA.host=tomcat worker.tomcatA.port=8009 EOF\rCreate mod_jk.conf cat \u003c\u003c EOF \u003e mod_jk.conf LoadModule jk_module modules/mod_jk.so JkWorkersFile conf.d/worker.properties JkMount /student tomcatA JkMount /student/* tomcatA EOF\rCreate docker-compose.yml cat \u003c\u003c EOF \u003e docker-compose.yml version: '3.8' services: mariadb: image: mariadb:10.5 container_name: mariadb environment: MYSQL_ROOT_PASSWORD: rootpass MYSQL_DATABASE: studentapp MYSQL_USER: student MYSQL_PASSWORD: student@1 networks: - app-network volumes: - mariadb_data:/var/lib/mysql tomcat: image: tomcat:8.5 container_name: tomcat depends_on: - mariadb ports: - \"8080:8080\" volumes: - ./context.xml:/usr/local/tomcat/conf/context.xml - ./mysql-connector-java-5.1.40.jar:/usr/local/tomcat/lib/mysql-connector-java-5.1.40.jar - ./student.war:/usr/local/tomcat/webapps/student.war networks: - app-network httpd: image: httpd:2.4 container_name: httpd depends_on: - tomcat ports: - \"80:80\" volumes: - ./worker.properties:/etc/httpd/conf.d/worker.properties:ro - ./mod_jk.conf:/etc/httpd/conf.d/mod_jk.conf:ro networks: - app-network networks: app-network: driver: bridge volumes: mariadb_data: EOF\rRun the setup docker-compose up -d\nShow running containers docker ps",
    "description": "——————————- 1. DB Server Configuration ——————————- yum install mariadb-server -y systemctl enable mariadb systemctl start mariadb\rConfigure Database mysql -e \" CREATE DATABASE studentapp; USE studentapp; CREATE TABLE Students ( student_id INT NOT NULL AUTO_INCREMENT, student_name VARCHAR(100) NOT NULL, student_addr VARCHAR(100) NOT NULL, student_age VARCHAR(3) NOT NULL, student_qual VARCHAR(20) NOT NULL, student_percent VARCHAR(10) NOT NULL, student_year_passed VARCHAR(10) NOT NULL, PRIMARY KEY (student_id) ); GRANT ALL PRIVILEGES ON studentapp.* TO 'student'@'%' IDENTIFIED BY 'student@1'; FLUSH PRIVILEGES; \"\r——————————- 2. Tomcat (Application Server) ——————————- yum install java -y cd /root wget -qO- http://www-us.apache.org/dist/tomcat/tomcat-8/v8.5.27/bin/apache-tomcat-8.5.27.tar.gz | tar -xz cd apache-tomcat-8.5.27 rm -rf webapps/* wget https://github.com/cit-latex/stack/raw/master/mysql-connector-java-5.1.40.jar -O lib/mysql-connector-java-5.1.40.jar wget https://github.com/cit-latex/stack/raw/master/student.war -O webapps/student.war\rEdit context.xml (replace accordingly) sed -i '/\u003c\\/Context\u003e/i \\\u003cResource name=\"jdbc/TestDB\" auth=\"Container\" type=\"javax.sql.DataSource\" maxTotal=\"100\" maxIdle=\"30\" maxWaitMillis=\"10000\" username=\"student\" password=\"student@1\" driverClassName=\"com.mysql.jdbc.Driver\" url=\"jdbc:mysql://\u003cIP-ADDRESS-OF-DB-SERVER\u003e:3306/studentapp\"/\u003e' conf/context.xml ### Start Tomcat sh bin/startup.sh\r——————————- 3. Web Server (Apache HTTPD) ——————————- yum install httpd httpd-devel gcc -y cd /root wget -qO- http://www-eu.apache.org/dist/tomcat/tomcat-connectors/jk/tomcat-connectors-1.2.42-src.tar.gz | tar -xz cd tomcat-connectors-1.2.42-src/native ./configure --with-apxs=/usr/bin/apxs make make install ## Configure worker.properties (replace \u003cIP-ADDRESS-OF-TOMCAT-SERVER\u003e accordingly) cat \u003c\u003c EOF \u003e /etc/httpd/conf.d/worker.properties worker.list=tomcatA worker.tomcatA.type=ajp13 worker.tomcatA.host=\u003cIP-ADDRESS-OF-TOMCAT-SERVER\u003e worker.tomcatA.port=8009 EOF ### Configure mod_jk.conf cat \u003c\u003c EOF \u003e /etc/httpd/conf.d/mod_jk.conf LoadModule jk_module modules/mod_jk.so JkWorkersFile conf.d/worker.properties JkMount /student tomcatA JkMount /student/* tomcatA EOF ### Enable and start HTTPD systemctl enable httpd systemctl start httpd\r3 tier App in conatiners using docker compose Create necessary directory mkdir -p docker-app \u0026\u0026 cd docker-app\rCreate context.xml cat \u003c\u003c EOF \u003e context.xml \u003cContext\u003e \u003cResource name=\"jdbc/TestDB\" auth=\"Container\" type=\"javax.sql.DataSource\" maxTotal=\"100\" maxIdle=\"30\" maxWaitMillis=\"10000\" username=\"student\" password=\"student@1\" driverClassName=\"com.mysql.jdbc.Driver\" url=\"jdbc:mysql://mariadb:3306/studentapp\"/\u003e \u003c/Context\u003e EOF\rDownload mysql connector wget https://github.com/cit-latex/stack/raw/master/mysql-connector-java-5.1.40.jar -O mysql-connector-java-5.1.40.jar\rDownload student.war wget https://github.com/cit-latex/stack/raw/master/student.war -O student.war\rCreate worker.properties cat \u003c\u003c EOF \u003e worker.properties worker.list=tomcatA worker.tomcatA.type=ajp13 worker.tomcatA.host=tomcat worker.tomcatA.port=8009 EOF\rCreate mod_jk.conf cat \u003c\u003c EOF \u003e mod_jk.conf LoadModule jk_module modules/mod_jk.so JkWorkersFile conf.d/worker.properties JkMount /student tomcatA JkMount /student/* tomcatA EOF\rCreate docker-compose.yml cat \u003c\u003c EOF \u003e docker-compose.yml version: '3.8' services: mariadb: image: mariadb:10.5 container_name: mariadb environment: MYSQL_ROOT_PASSWORD: rootpass MYSQL_DATABASE: studentapp MYSQL_USER: student MYSQL_PASSWORD: student@1 networks: - app-network volumes: - mariadb_data:/var/lib/mysql tomcat: image: tomcat:8.5 container_name: tomcat depends_on: - mariadb ports: - \"8080:8080\" volumes: - ./context.xml:/usr/local/tomcat/conf/context.xml - ./mysql-connector-java-5.1.40.jar:/usr/local/tomcat/lib/mysql-connector-java-5.1.40.jar - ./student.war:/usr/local/tomcat/webapps/student.war networks: - app-network httpd: image: httpd:2.4 container_name: httpd depends_on: - tomcat ports: - \"80:80\" volumes: - ./worker.properties:/etc/httpd/conf.d/worker.properties:ro - ./mod_jk.conf:/etc/httpd/conf.d/mod_jk.conf:ro networks: - app-network networks: app-network: driver: bridge volumes: mariadb_data: EOF\rRun the setup docker-compose up -d",
    "tags": [],
    "title": "Project 01",
    "uri": "/containers/project01.md/index.html"
  },
  {
    "breadcrumb": "Containers \u003e Docker",
    "content": "Projects Use Nexus as a Docker Registry Task 01: Nexus as a Container Registry Objectives: Install Nexus as a Docker image, login, and push images.\nSolution Step 1: Run Nexus in Docker mkdir -p ~/nexus-data \u0026\u0026 chmod 777 ~/nexus-data docker run -d --name nexus -p 8081:8081 -p 5000:5000 -v ~/nexus-data:/nexus-data sonatype/nexus3 Access Nexus UI: http://localhost:8081\nDefault admin credentials: admin / password in ~/nexus-data/admin.password\nStep 2: Create a Private Docker Registry in Nexus\n1. Login to Nexus UI (http://localhost:8081) 2. Go to Administration → Repositories → Create repository 3. Select docker (hosted) 4. Name it docker-hosted 5. Set HTTP Port to 5000 6. Save Nexus now acts as a Docker registry at http://localhost:5000\nStep 3: Configure Docker to Trust Nexus Registry\nEdit /etc/docker/daemon.json: { \"insecure-registries\": [\"localhost:5000\"] } Restart Docker: sudo systemctl restart docker Step 4: Login to Nexus Docker Registry\ndocker login localhost:5000 (enter Nexus admin credentials)\nStep 5: Tag and Push an Image to Nexus\ndocker pull alpine:latest docker tag alpine:latest localhost:5000/my-alpine:1.0 docker push localhost:5000/my-alpine:1.0 Step 6: Verify Image in Nexus Go to Nexus UI → Browse → Repositories → docker-hosted You should see my-alpine:1.0 successfully pushed Popular Docker Registries: Docker Hub (public) Harbor (open-source private), JFrog Artifactory (enterprise), GitLab Container Registry (integrated), AWS ECR Azure ACR Google Artifact Registry/GCR",
    "description": "Projects Use Nexus as a Docker Registry Task 01: Nexus as a Container Registry Objectives: Install Nexus as a Docker image, login, and push images.\nSolution Step 1: Run Nexus in Docker mkdir -p ~/nexus-data \u0026\u0026 chmod 777 ~/nexus-data docker run -d --name nexus -p 8081:8081 -p 5000:5000 -v ~/nexus-data:/nexus-data sonatype/nexus3 Access Nexus UI: http://localhost:8081\nDefault admin credentials: admin / password in ~/nexus-data/admin.password\nStep 2: Create a Private Docker Registry in Nexus",
    "tags": [],
    "title": "Projects",
    "uri": "/containers/projects/index.html"
  },
  {
    "breadcrumb": "Containers",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Containers",
    "content": "Docker – Table of Contents 1. Linux Kernel \u0026 Container Fundamentals Namespaces → PID, Mount, Network, IPC, UTS, User cgroups → CPU, Memory, I/O, PIDs limits Filesystem isolation with chroot Experimenting with unshare to create isolated environments How Docker and Podman use namespaces and cgroups internally 2. Introduction What is Docker? Benefits of containerization Docker vs Virtual Machines Containers vs Low-Level Runtimes (CRI-O, containerd) 3. Installation \u0026 Setup Install Docker on Linux, Windows, macOS Docker Desktop Docker Engine vs Docker Desktop Verify installation 4. Docker Architecture Docker Daemon Docker Client Docker Images Docker Containers Docker Registries (Docker Hub, private registries) Low-level container runtimes: CRI-O \u0026 containerd 5. Working with Docker Images Building images with Dockerfile Using prebuilt images Managing images (docker pull, docker images, docker rmi) Image layers \u0026 storage drivers 6. Working with Containers Running containers (docker run) Listing, starting, stopping, removing containers Detached mode \u0026 interactive mode Executing commands inside containers Understanding isolation using namespaces Resource control using cgroups Low-level experiments with unshare and chroot 7. Docker Networking Bridge network Host network Overlay network Custom networks Port mapping \u0026 exposing services Network namespaces overview 8. Docker Volumes \u0026 Storage Bind mounts vs volumes Creating and using volumes Sharing data between containers Backup \u0026 restore volumes Storage namespaces \u0026 container filesystem isolation 9. Docker Compose Introduction to Compose docker-compose.yml structure Multi-container applications Environment variables \u0026 scaling 10. Docker Security Best practices User namespaces Scanning images Secrets management Seccomp \u0026 AppArmor profiles 11. Docker in CI/CD Using Docker in pipelines Building and pushing images Deploying with Docker Integration with Kubernetes using CRI-O/containerd 12. Advanced Docker Multi-stage builds Health checks Resource limits (CPU \u0026 memory) Logging \u0026 monitoring Low-level runtime configuration 13. Troubleshooting Common errors Debugging containers Checking logs \u0026 events Using docker inspect and runtime debug tools 14. Docker vs Alternatives Podman LXC/LXD CRI-O \u0026 containerd",
    "description": "Docker – Table of Contents 1. Linux Kernel \u0026 Container Fundamentals Namespaces → PID, Mount, Network, IPC, UTS, User cgroups → CPU, Memory, I/O, PIDs limits Filesystem isolation with chroot Experimenting with unshare to create isolated environments How Docker and Podman use namespaces and cgroups internally 2. Introduction What is Docker? Benefits of containerization Docker vs Virtual Machines Containers vs Low-Level Runtimes (CRI-O, containerd) 3. Installation \u0026 Setup Install Docker on Linux, Windows, macOS Docker Desktop Docker Engine vs Docker Desktop Verify installation 4. Docker Architecture Docker Daemon Docker Client Docker Images Docker Containers Docker Registries (Docker Hub, private registries) Low-level container runtimes: CRI-O \u0026 containerd 5. Working with Docker Images Building images with Dockerfile Using prebuilt images Managing images (docker pull, docker images, docker rmi) Image layers \u0026 storage drivers 6. Working with Containers Running containers (docker run) Listing, starting, stopping, removing containers Detached mode \u0026 interactive mode Executing commands inside containers Understanding isolation using namespaces Resource control using cgroups Low-level experiments with unshare and chroot 7. Docker Networking Bridge network Host network Overlay network Custom networks Port mapping \u0026 exposing services Network namespaces overview 8. Docker Volumes \u0026 Storage Bind mounts vs volumes Creating and using volumes Sharing data between containers Backup \u0026 restore volumes Storage namespaces \u0026 container filesystem isolation 9. Docker Compose Introduction to Compose docker-compose.yml structure Multi-container applications Environment variables \u0026 scaling 10. Docker Security Best practices User namespaces Scanning images Secrets management Seccomp \u0026 AppArmor profiles 11. Docker in CI/CD Using Docker in pipelines Building and pushing images Deploying with Docker Integration with Kubernetes using CRI-O/containerd 12. Advanced Docker Multi-stage builds Health checks Resource limits (CPU \u0026 memory) Logging \u0026 monitoring Low-level runtime configuration 13. Troubleshooting Common errors Debugging containers Checking logs \u0026 events Using docker inspect and runtime debug tools 14. Docker vs Alternatives Podman LXC/LXD CRI-O \u0026 containerd",
    "tags": [],
    "title": "Docker",
    "uri": "/containers/index.html"
  },
  {
    "breadcrumb": "Containers",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
